{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376d1485",
   "metadata": {},
   "source": [
    "# TP3 - Find the soda logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdaf884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467ba69",
   "metadata": {},
   "source": [
    "## Find the soda logo within the provided images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20eb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399ff4b",
   "metadata": {},
   "source": [
    "1. Obtain a logo detection in each image without false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_image(path, max_size=1200):\n",
    "    \"\"\"Load image and return RGB, grayscale, and BGR versions.\"\"\"\n",
    "    img = cv.imread(path)\n",
    "    h, w = img.shape[:2]\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / max(h, w)\n",
    "        img = cv.resize(img, None, fx=scale, fy=scale)\n",
    "    \n",
    "    img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    return img_rgb, img_gray, img\n",
    "\n",
    "\n",
    "def load_template(path, max_size=400):\n",
    "    \"\"\"Load template as grayscale.\"\"\"\n",
    "    template = cv.imread(path, 0)\n",
    "    h, w = template.shape[:2]\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / max(h, w)\n",
    "        template = cv.resize(template, None, fx=scale, fy=scale)\n",
    "    return template\n",
    "\n",
    "\n",
    "# Note: The create_template_variants function is defined in Cell 5 (DETECTION FUNCTIONS)\n",
    "# This cell only contains utility functions for loading and visualization\n",
    "\n",
    "\n",
    "def draw_detections(img_rgb, detections):\n",
    "    \"\"\"Draw detection rectangles on image with confidence scores.\"\"\"\n",
    "    img_out = img_rgb.copy()\n",
    "    for det in detections:\n",
    "        x, y, w, h = det[:4]\n",
    "        score = det[4] if len(det) > 4 else 1.0\n",
    "        \n",
    "        # Color according to confidence level (green high, yellow medium, red low)\n",
    "        if score >= 0.7:\n",
    "            color = (0, 255, 0)  # Green\n",
    "        elif score >= 0.5:\n",
    "            color = (255, 255, 0)  # Yellow\n",
    "        else:\n",
    "            color = (255, 0, 0)  # Red\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv.rectangle(img_out, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Display confidence level\n",
    "        label = f'{score:.2f}'\n",
    "        label_size, _ = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        label_y = max(y - 5, label_size[1])\n",
    "        cv.rectangle(img_out, (x, label_y - label_size[1] - 5), \n",
    "                    (x + label_size[0], label_y + 5), color, -1)\n",
    "        cv.putText(img_out, label, (x, label_y), \n",
    "                  cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "    return img_out\n",
    "\n",
    "\n",
    "def plot_results_grid(results, title, save_path=None):\n",
    "    \"\"\"Plot results in a grid layout with confidence information.\"\"\"\n",
    "    n = len(results)\n",
    "    n_cols = 3\n",
    "    n_rows = (n + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (img_rgb, detections, img_name) in enumerate(results):\n",
    "        img_result = draw_detections(img_rgb, detections)\n",
    "        axes[idx].imshow(img_result)\n",
    "        \n",
    "        # Display confidence information in the title\n",
    "        if detections:\n",
    "            scores = [d[4] for d in detections if len(d) > 4]\n",
    "            avg_conf = np.mean(scores) if scores else 0.0\n",
    "            conf_str = f\"avg confidence: {avg_conf:.2f}\" if len(detections) > 1 else f\"confidence: {scores[0]:.2f}\"\n",
    "            axes[idx].set_title(f\"{img_name}\\n{len(detections)} detection(s), {conf_str}\")\n",
    "        else:\n",
    "            axes[idx].set_title(f\"{img_name}\\n0 detections\")\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    for idx in range(n, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_single_result(img_rgb, detections, title, save_path=None):\n",
    "    \"\"\"Plot single image result with confidence information.\"\"\"\n",
    "    img_result = draw_detections(img_rgb, detections)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(img_result)\n",
    "    \n",
    "    # Display confidence information in the title\n",
    "    if detections:\n",
    "        scores = [d[4] for d in detections if len(d) > 4]\n",
    "        avg_conf = np.mean(scores) if scores else 0.0\n",
    "        min_conf = np.min(scores) if scores else 0.0\n",
    "        max_conf = np.max(scores) if scores else 0.0\n",
    "        title_with_conf = f\"{title} | {len(detections)} detections\\nConfidence: min={min_conf:.2f}, avg={avg_conf:.2f}, max={max_conf:.2f}\"\n",
    "    else:\n",
    "        title_with_conf = f\"{title} | 0 detections\"\n",
    "    \n",
    "    plt.title(title_with_conf, fontsize=12)\n",
    "    plt.axis('off')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=120, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ad9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETECTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_template_variants(template):\n",
    "    \"\"\"\n",
    "    Create useful template variants for template matching.\n",
    "    Preprocessing matches image preprocessing for better alignment.\n",
    "    Optimized: removed redundant variants to improve performance.\n",
    "    \"\"\"\n",
    "    # Apply CLAHE first (matching image preprocessing)\n",
    "    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_template = clahe.apply(template)\n",
    "    \n",
    "    # Smooth variant (with CLAHE applied)\n",
    "    smooth = cv.GaussianBlur(clahe_template, (3, 3), 0)\n",
    "    \n",
    "    # Inverted variant (with CLAHE applied)\n",
    "    inverted = 255 - clahe_template\n",
    "    \n",
    "    # Return only essential variants (removed redundant 'original' and 'equalized' for speed)\n",
    "    return {\n",
    "        'clahe': clahe_template,  # Base variant matching image preprocessing\n",
    "        'smooth': smooth,          # Helps with slight blur variations\n",
    "        'inverted': inverted,      # Helps with contrast inversions\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_image(img_gray):\n",
    "    \"\"\"\n",
    "    Preprocess image before template matching.\n",
    "    Uses CLAHE to match template preprocessing for better alignment.\n",
    "    \"\"\"\n",
    "    # Apply CLAHE for contrast normalization (matching template preprocessing)\n",
    "    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img = clahe.apply(img_gray)\n",
    "    # Light Gaussian blur to reduce noise while preserving edges\n",
    "    img = cv.GaussianBlur(img, (3, 3), 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes (x, y, w, h).\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    \n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "    inter_w = max(0, xi2 - xi1)\n",
    "    inter_h = max(0, yi2 - yi1)\n",
    "    inter_area = inter_w * inter_h\n",
    "    \n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "\n",
    "def nms_global(detections, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression globally across all detections.\n",
    "    detections: list of (x, y, w, h, score)\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    detections = sorted(detections, key=lambda d: d[4], reverse=True)\n",
    "    \n",
    "    keep = []\n",
    "    while detections:\n",
    "        best = detections.pop(0)\n",
    "        keep.append(best)\n",
    "        detections = [d for d in detections if compute_iou(best[:4], d[:4]) < iou_threshold]\n",
    "    \n",
    "    return keep\n",
    "\n",
    "\n",
    "def find_by_template(img_gray, template_variants, single_detection=True, min_threshold=0.30):\n",
    "    \"\"\"\n",
    "    Multi-scale template matching - OPTIMIZED VERSION.\n",
    "    - Reduced to 20 scales (from 30) for better performance\n",
    "    - Optimized threshold calculation using numpy operations\n",
    "    - Reduced template variants (3 instead of 5)\n",
    "    - Early filtering to reduce unnecessary operations\n",
    "    \n",
    "    For multiple detections, uses much lower threshold to catch all instances.\n",
    "    Following 01.Templates.ipynb Cell 6 approach but adapted for multi-scale.\n",
    "    \"\"\"\n",
    "    h, w = img_gray.shape\n",
    "    img_processed = preprocess_image(img_gray)\n",
    "    \n",
    "    # Adjust threshold according to detection mode\n",
    "    # For multiple detections, use much more permissive thresholds\n",
    "    if single_detection:\n",
    "        base_threshold = 0.25  # For single detection, keep moderate threshold\n",
    "        adaptive_multiplier = 1.5\n",
    "    else:\n",
    "        # For multiple detections: much lower threshold to catch all instances\n",
    "        base_threshold = 0.15  # Very low threshold to catch all instances\n",
    "        adaptive_multiplier = 0.3  # Very permissive multiplier\n",
    "    \n",
    "    # Reduced scales from 30 to 20 for better performance (still covers range 0.2-1.6)\n",
    "    scales = np.linspace(0.2, 1.6, 20)\n",
    "    all_detections = []\n",
    "    \n",
    "    # Pre-calculate aspect ratio bounds to avoid repeated calculations\n",
    "    aspect_min, aspect_max = 0.5, 3.0\n",
    "    \n",
    "    for scale in scales:\n",
    "        for name, tmpl in template_variants.items():\n",
    "            th, tw = tmpl.shape\n",
    "            new_w = int(tw * scale)\n",
    "            new_h = int(th * scale)\n",
    "            \n",
    "            # Early filtering: skip invalid sizes\n",
    "            if new_w > w or new_h > h or new_w < 20 or new_h < 10:\n",
    "                continue\n",
    "            \n",
    "            # Check aspect ratio early to avoid expensive template matching\n",
    "            aspect = new_w / new_h if new_h > 0 else 0\n",
    "            if not (aspect_min < aspect < aspect_max):\n",
    "                continue\n",
    "            \n",
    "            scaled_tmpl = cv.resize(tmpl, (new_w, new_h))\n",
    "            # Use TM_CCOEFF_NORMED as in 01.Templates.ipynb Cell 6\n",
    "            res = cv.matchTemplate(img_processed, scaled_tmpl, cv.TM_CCOEFF_NORMED)\n",
    "            \n",
    "            # Optimized threshold calculation using numpy\n",
    "            if single_detection:\n",
    "                # Adaptive threshold for single detection\n",
    "                mean_val = res.mean()\n",
    "                std_val = res.std()\n",
    "                threshold = max(mean_val + adaptive_multiplier * std_val, base_threshold)\n",
    "            else:\n",
    "                # For multiple detections: use more permissive approach\n",
    "                # Optimized: use numpy operations directly\n",
    "                mean_val = res.mean()\n",
    "                std_val = res.std()\n",
    "                adaptive_threshold = mean_val + adaptive_multiplier * std_val\n",
    "                max_allowed_threshold = 0.35  # Cap for multiple detections\n",
    "                threshold = min(adaptive_threshold, max_allowed_threshold)\n",
    "                threshold = max(threshold, base_threshold)  # Ensure minimum threshold\n",
    "            \n",
    "            # np.where() as in 01.Templates.ipynb Cell 6\n",
    "            # Optimized: directly get scores where threshold is met\n",
    "            locations = np.where(res >= threshold)\n",
    "            \n",
    "            # Batch process locations for better performance\n",
    "            for pt in zip(*locations[::-1]):\n",
    "                x, y = pt\n",
    "                score = res[y, x]\n",
    "                all_detections.append((x, y, new_w, new_h, score))\n",
    "    \n",
    "    if not all_detections:\n",
    "        return (None, None, 0) if single_detection else []\n",
    "    \n",
    "    # Apply NMS to filter overlaps\n",
    "    iou_thresh = 0.25 if single_detection else 0.35\n",
    "    final_detections = nms_global(all_detections, iou_threshold=iou_thresh)\n",
    "    \n",
    "    if single_detection:\n",
    "        best = final_detections[0]\n",
    "        return (best[0], best[1], best[2], best[3]), 'template', best[4]\n",
    "    \n",
    "    # For multiple detections, filter out very low confidence detections\n",
    "    if final_detections:\n",
    "        min_confidence = 0.30\n",
    "        final_detections = [d for d in final_detections if d[4] >= min_confidence]\n",
    "    \n",
    "    return final_detections\n",
    "\n",
    "\n",
    "def detect_logo(img_bgr, img_gray, template_variants, single_detection=True):\n",
    "    \"\"\"\n",
    "    Detection pipeline using only template matching (aligned with 01.Templates.ipynb).\n",
    "    - Single detection: Template matching with strict threshold\n",
    "    - Multi detection: Template matching with more permissive threshold (like 01.Templates.ipynb Cell 6)\n",
    "    \"\"\"\n",
    "    if single_detection:\n",
    "        bbox, method, score = find_by_template(img_gray, template_variants, single_detection=True)\n",
    "        if bbox:\n",
    "            return bbox, method, score\n",
    "        return None, None, 0\n",
    "    else:\n",
    "        # Multiple detections using template matching (like 01.Templates.ipynb Cell 6)\n",
    "        # Lower threshold for multiple detections to catch all instances\n",
    "        detections = find_by_template(img_gray, template_variants, single_detection=False, min_threshold=0.35)\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "up51dq952i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEMPLATE VARIANTS VISUALIZATION (3 optimized variants)\n",
    "# =============================================================================\n",
    "\n",
    "TEMPLATE_PATH = 'template/pattern.png'\n",
    "template = load_template(TEMPLATE_PATH)\n",
    "variants = create_template_variants(template)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(variants), figsize=(6 * len(variants), 4))\n",
    "\n",
    "for ax, (name, img) in zip(axes, variants.items()):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle(f'Template Variants ({len(variants)} optimized variants)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/template_variants.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af173e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCA-COLA-LOGO.jpg: template (confidence=0.428)\n",
      "coca_logo_1.png: template (confidence=0.379)\n",
      "coca_logo_2.png: template (confidence=0.405)\n",
      "coca_multi.png: template (confidence=0.480)\n",
      "coca_retro_1.png: template (confidence=0.653)\n",
      "coca_retro_2.png: template (confidence=0.486)\n",
      "logo_1.png: template (confidence=0.480)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ASSIGNMENT 1: Single Detection per Image\n",
    "# =============================================================================\n",
    "\n",
    "IMAGES_DIR = 'images/'\n",
    "image_files = sorted([f for f in os.listdir(IMAGES_DIR) if f.endswith(('.png', '.jpg'))])\n",
    "\n",
    "results = []\n",
    "for img_name in image_files:\n",
    "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "    img_rgb, img_gray, img_bgr = load_image(img_path)\n",
    "    \n",
    "    bbox, method, match_val = detect_logo(img_bgr, img_gray, variants, single_detection=True)\n",
    "    \n",
    "    if bbox:\n",
    "        x, y, w, h = bbox\n",
    "        # Save with real confidence score\n",
    "        detections = [(x, y, w, h, match_val)]\n",
    "        print(f\"{img_name}: {method} (confidence={match_val:.3f})\")\n",
    "    else:\n",
    "        detections = []\n",
    "        print(f\"{img_name}: NOT DETECTED\")\n",
    "    \n",
    "    results.append((img_rgb, detections, img_name))\n",
    "\n",
    "plot_results_grid(results, \"ASSIGNMENT 1: Single Detection per Image\", 'results/Figure_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891386f8",
   "metadata": {},
   "source": [
    "2. Propose and validate an algorithm for multiple detections in the image coca_multi.png using the same template from item 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gktc1c4y0n7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 226 logo instances in coca_multi.png\n",
      "  Detection 1: position=(403,261), size=(138x60), confidence=0.480\n",
      "  Detection 2: position=(400,547), size=(109x47), confidence=0.479\n",
      "  Detection 3: position=(270,536), size=(138x60), confidence=0.478\n",
      "  Detection 4: position=(504,261), size=(138x60), confidence=0.475\n",
      "  Detection 5: position=(533,537), size=(138x60), confidence=0.474\n",
      "  Detection 6: position=(269,261), size=(138x60), confidence=0.473\n",
      "  Detection 7: position=(614,536), size=(138x60), confidence=0.472\n",
      "  Detection 8: position=(457,537), size=(138x60), confidence=0.471\n",
      "  Detection 9: position=(589,262), size=(138x60), confidence=0.466\n",
      "  Detection 10: position=(203,536), size=(138x60), confidence=0.465\n",
      "  Detection 11: position=(49,533), size=(138x60), confidence=0.464\n",
      "  Detection 12: position=(173,261), size=(138x60), confidence=0.460\n",
      "  Detection 13: position=(137,535), size=(138x60), confidence=0.459\n",
      "  Detection 14: position=(93,261), size=(138x60), confidence=0.455\n",
      "  Detection 15: position=(13,260), size=(138x60), confidence=0.452\n",
      "  Detection 16: position=(0,542), size=(109x47), confidence=0.449\n",
      "  Detection 17: position=(416,14), size=(168x73), confidence=0.446\n",
      "  Detection 18: position=(661,262), size=(138x60), confidence=0.443\n",
      "  Detection 19: position=(679,546), size=(109x47), confidence=0.441\n",
      "  Detection 20: position=(335,262), size=(138x60), confidence=0.440\n",
      "  Detection 21: position=(597,226), size=(80x35), confidence=0.440\n",
      "  Detection 22: position=(211,3), size=(197x86), confidence=0.439\n",
      "  Detection 23: position=(329,13), size=(168x73), confidence=0.439\n",
      "  Detection 24: position=(579,16), size=(168x73), confidence=0.435\n",
      "  Detection 25: position=(339,547), size=(109x47), confidence=0.433\n",
      "  Detection 26: position=(706,153), size=(80x35), confidence=0.431\n",
      "  Detection 27: position=(500,14), size=(168x73), confidence=0.428\n",
      "  Detection 28: position=(32,150), size=(80x35), confidence=0.424\n",
      "  Detection 29: position=(428,450), size=(286x125), confidence=0.421\n",
      "  Detection 30: position=(137,228), size=(80x35), confidence=0.415\n",
      "  Detection 31: position=(41,6), size=(197x86), confidence=0.415\n",
      "  Detection 32: position=(654,428), size=(80x35), confidence=0.413\n",
      "  Detection 33: position=(201,154), size=(80x35), confidence=0.412\n",
      "  Detection 34: position=(236,432), size=(80x35), confidence=0.411\n",
      "  Detection 35: position=(468,271), size=(109x47), confidence=0.410\n",
      "  Detection 36: position=(465,285), size=(315x138), confidence=0.409\n",
      "  Detection 37: position=(243,443), size=(315x138), confidence=0.409\n",
      "  Detection 38: position=(478,227), size=(80x35), confidence=0.408\n",
      "  Detection 39: position=(581,36), size=(80x35), confidence=0.407\n",
      "  Detection 40: position=(281,146), size=(80x35), confidence=0.406\n",
      "  Detection 41: position=(75,425), size=(80x35), confidence=0.406\n",
      "  Detection 42: position=(489,427), size=(80x35), confidence=0.404\n",
      "  Detection 43: position=(31,281), size=(345x151), confidence=0.403\n",
      "  Detection 44: position=(84,450), size=(286x125), confidence=0.403\n",
      "  Detection 45: position=(221,228), size=(80x35), confidence=0.402\n",
      "  Detection 46: position=(542,154), size=(80x35), confidence=0.401\n",
      "  Detection 47: position=(37,162), size=(315x138), confidence=0.400\n",
      "  Detection 48: position=(260,226), size=(80x35), confidence=0.400\n",
      "  Detection 49: position=(407,433), size=(80x35), confidence=0.400\n",
      "  Detection 50: position=(0,420), size=(80x35), confidence=0.399\n",
      "  Detection 51: position=(556,226), size=(80x35), confidence=0.399\n",
      "  Detection 52: position=(283,281), size=(345x151), confidence=0.399\n",
      "  Detection 53: position=(409,35), size=(80x35), confidence=0.398\n",
      "  Detection 54: position=(676,227), size=(80x35), confidence=0.397\n",
      "  Detection 55: position=(664,302), size=(109x47), confidence=0.396\n",
      "  Detection 56: position=(602,480), size=(197x86), confidence=0.395\n",
      "  Detection 57: position=(570,431), size=(80x35), confidence=0.395\n",
      "  Detection 58: position=(625,152), size=(80x35), confidence=0.395\n",
      "  Detection 59: position=(60,228), size=(80x35), confidence=0.395\n",
      "  Detection 60: position=(14,226), size=(80x35), confidence=0.395\n",
      "  Detection 61: position=(22,477), size=(197x86), confidence=0.393\n",
      "  Detection 62: position=(114,151), size=(80x35), confidence=0.393\n",
      "  Detection 63: position=(181,228), size=(80x35), confidence=0.390\n",
      "  Detection 64: position=(330,63), size=(109x47), confidence=0.389\n",
      "  Detection 65: position=(369,146), size=(80x35), confidence=0.387\n",
      "  Detection 66: position=(271,422), size=(138x60), confidence=0.386\n",
      "  Detection 67: position=(317,556), size=(80x35), confidence=0.384\n",
      "  Detection 68: position=(715,227), size=(80x35), confidence=0.384\n",
      "  Detection 69: position=(204,161), size=(315x138), confidence=0.384\n",
      "  Detection 70: position=(262,71), size=(80x35), confidence=0.383\n",
      "  Detection 71: position=(573,93), size=(109x47), confidence=0.383\n",
      "  Detection 72: position=(471,170), size=(286x125), confidence=0.383\n",
      "  Detection 73: position=(98,229), size=(80x35), confidence=0.382\n",
      "  Detection 74: position=(156,429), size=(80x35), confidence=0.381\n",
      "  Detection 75: position=(409,234), size=(227x99), confidence=0.380\n",
      "  Detection 76: position=(296,90), size=(109x47), confidence=0.380\n",
      "  Detection 77: position=(393,226), size=(80x35), confidence=0.380\n",
      "  Detection 78: position=(633,503), size=(80x35), confidence=0.379\n",
      "  Detection 79: position=(406,54), size=(138x60), confidence=0.379\n",
      "  Detection 80: position=(701,76), size=(80x35), confidence=0.379\n",
      "  Detection 81: position=(612,74), size=(80x35), confidence=0.378\n",
      "  Detection 82: position=(637,227), size=(80x35), confidence=0.378\n",
      "  Detection 83: position=(498,94), size=(109x47), confidence=0.376\n",
      "  Detection 84: position=(379,191), size=(227x99), confidence=0.375\n",
      "  Detection 85: position=(616,182), size=(80x35), confidence=0.375\n",
      "  Detection 86: position=(470,505), size=(80x35), confidence=0.374\n",
      "  Detection 87: position=(96,300), size=(109x47), confidence=0.373\n",
      "  Detection 88: position=(327,280), size=(80x35), confidence=0.373\n",
      "  Detection 89: position=(496,35), size=(80x35), confidence=0.372\n",
      "  Detection 90: position=(556,72), size=(80x35), confidence=0.372\n",
      "  Detection 91: position=(208,501), size=(138x60), confidence=0.372\n",
      "  Detection 92: position=(449,149), size=(80x35), confidence=0.371\n",
      "  Detection 93: position=(152,234), size=(256x112), confidence=0.371\n",
      "  Detection 94: position=(274,556), size=(80x35), confidence=0.370\n",
      "  Detection 95: position=(443,45), size=(197x86), confidence=0.370\n",
      "  Detection 96: position=(174,300), size=(109x47), confidence=0.370\n",
      "  Detection 97: position=(13,66), size=(109x47), confidence=0.368\n",
      "  Detection 98: position=(431,227), size=(80x35), confidence=0.367\n",
      "  Detection 99: position=(316,431), size=(80x35), confidence=0.366\n",
      "  Detection 100: position=(49,172), size=(80x35), confidence=0.365\n",
      "  Detection 101: position=(587,302), size=(109x47), confidence=0.364\n",
      "  Detection 102: position=(286,233), size=(256x112), confidence=0.364\n",
      "  Detection 103: position=(426,301), size=(109x47), confidence=0.364\n",
      "  Detection 104: position=(505,302), size=(109x47), confidence=0.363\n",
      "  Detection 105: position=(364,89), size=(109x47), confidence=0.363\n",
      "  Detection 106: position=(503,234), size=(256x112), confidence=0.362\n",
      "  Detection 107: position=(230,555), size=(80x35), confidence=0.361\n",
      "  Detection 108: position=(142,10), size=(168x73), confidence=0.361\n",
      "  Detection 109: position=(451,557), size=(80x35), confidence=0.361\n",
      "  Detection 110: position=(252,300), size=(109x47), confidence=0.361\n",
      "  Detection 111: position=(495,557), size=(80x35), confidence=0.361\n",
      "  Detection 112: position=(183,554), size=(80x35), confidence=0.360\n",
      "  Detection 113: position=(580,46), size=(197x86), confidence=0.360\n",
      "  Detection 114: position=(389,494), size=(168x73), confidence=0.360\n",
      "  Detection 115: position=(16,300), size=(109x47), confidence=0.359\n",
      "  Detection 116: position=(29,91), size=(109x47), confidence=0.358\n",
      "  Detection 117: position=(234,91), size=(109x47), confidence=0.358\n",
      "  Detection 118: position=(476,199), size=(80x35), confidence=0.357\n",
      "  Detection 119: position=(0,201), size=(197x86), confidence=0.357\n",
      "  Detection 120: position=(388,70), size=(80x35), confidence=0.357\n",
      "  Detection 121: position=(517,226), size=(80x35), confidence=0.356\n",
      "  Detection 122: position=(133,554), size=(80x35), confidence=0.352\n",
      "  Detection 123: position=(133,173), size=(80x35), confidence=0.352\n",
      "  Detection 124: position=(202,287), size=(256x112), confidence=0.352\n",
      "  Detection 125: position=(472,71), size=(80x35), confidence=0.352\n",
      "  Detection 126: position=(656,281), size=(80x35), confidence=0.351\n",
      "  Detection 127: position=(546,53), size=(138x60), confidence=0.351\n",
      "  Detection 128: position=(637,93), size=(109x47), confidence=0.351\n",
      "  Detection 129: position=(257,507), size=(80x35), confidence=0.351\n",
      "  Detection 130: position=(53,552), size=(80x35), confidence=0.351\n",
      "  Detection 131: position=(241,32), size=(80x35), confidence=0.350\n",
      "  Detection 132: position=(343,301), size=(109x47), confidence=0.350\n",
      "  Detection 133: position=(607,556), size=(80x35), confidence=0.350\n",
      "  Detection 134: position=(617,281), size=(80x35), confidence=0.350\n",
      "  Detection 135: position=(17,234), size=(256x112), confidence=0.350\n",
      "  Detection 136: position=(299,18), size=(109x47), confidence=0.349\n",
      "  Detection 137: position=(91,294), size=(197x86), confidence=0.349\n",
      "  Detection 138: position=(695,281), size=(80x35), confidence=0.349\n",
      "  Detection 139: position=(578,281), size=(80x35), confidence=0.348\n",
      "  Detection 140: position=(155,391), size=(256x112), confidence=0.347\n",
      "  Detection 141: position=(420,295), size=(197x86), confidence=0.347\n",
      "  Detection 142: position=(563,556), size=(80x35), confidence=0.347\n",
      "  Detection 143: position=(95,553), size=(80x35), confidence=0.347\n",
      "  Detection 144: position=(483,387), size=(256x112), confidence=0.346\n",
      "  Detection 145: position=(177,506), size=(80x35), confidence=0.346\n",
      "  Detection 146: position=(404,281), size=(80x35), confidence=0.345\n",
      "  Detection 147: position=(651,555), size=(80x35), confidence=0.344\n",
      "  Detection 148: position=(545,500), size=(138x60), confidence=0.344\n",
      "  Detection 149: position=(698,184), size=(80x35), confidence=0.344\n",
      "  Detection 150: position=(164,63), size=(109x47), confidence=0.343\n",
      "  Detection 151: position=(125,500), size=(138x60), confidence=0.342\n",
      "  Detection 152: position=(29,413), size=(138x60), confidence=0.341\n",
      "  Detection 153: position=(153,33), size=(80x35), confidence=0.341\n",
      "  Detection 154: position=(534,281), size=(80x35), confidence=0.341\n",
      "  Detection 155: position=(711,507), size=(80x35), confidence=0.340\n",
      "  Detection 156: position=(468,485), size=(168x73), confidence=0.339\n",
      "  Detection 157: position=(278,43), size=(197x86), confidence=0.338\n",
      "  Detection 158: position=(410,94), size=(109x47), confidence=0.338\n",
      "  Detection 159: position=(286,219), size=(109x47), confidence=0.338\n",
      "  Detection 160: position=(505,195), size=(168x73), confidence=0.338\n",
      "  Detection 161: position=(15,499), size=(80x35), confidence=0.337\n",
      "  Detection 162: position=(446,280), size=(80x35), confidence=0.337\n",
      "  Detection 163: position=(294,502), size=(138x60), confidence=0.336\n",
      "  Detection 164: position=(433,199), size=(80x35), confidence=0.336\n",
      "  Detection 165: position=(521,418), size=(138x60), confidence=0.336\n",
      "  Detection 166: position=(71,34), size=(80x35), confidence=0.335\n",
      "  Detection 167: position=(697,239), size=(80x35), confidence=0.335\n",
      "  Detection 168: position=(287,280), size=(80x35), confidence=0.334\n",
      "  Detection 169: position=(35,121), size=(168x73), confidence=0.334\n",
      "  Detection 170: position=(599,198), size=(80x35), confidence=0.334\n",
      "  Detection 171: position=(298,171), size=(80x35), confidence=0.334\n",
      "  Detection 172: position=(216,508), size=(80x35), confidence=0.332\n",
      "  Detection 173: position=(404,137), size=(138x60), confidence=0.332\n",
      "  Detection 174: position=(247,279), size=(80x35), confidence=0.332\n",
      "  Detection 175: position=(149,94), size=(109x47), confidence=0.332\n",
      "  Detection 176: position=(671,504), size=(80x35), confidence=0.331\n",
      "  Detection 177: position=(207,51), size=(138x60), confidence=0.331\n",
      "  Detection 178: position=(716,172), size=(80x35), confidence=0.330\n",
      "  Detection 179: position=(404,304), size=(80x35), confidence=0.330\n",
      "  Detection 180: position=(505,64), size=(109x47), confidence=0.330\n",
      "  Detection 181: position=(332,6), size=(80x35), confidence=0.330\n",
      "  Detection 182: position=(29,279), size=(80x35), confidence=0.330\n",
      "  Detection 183: position=(215,173), size=(80x35), confidence=0.328\n",
      "  Detection 184: position=(353,225), size=(80x35), confidence=0.328\n",
      "  Detection 185: position=(537,238), size=(80x35), confidence=0.328\n",
      "  Detection 186: position=(69,279), size=(80x35), confidence=0.327\n",
      "  Detection 187: position=(154,279), size=(80x35), confidence=0.327\n",
      "  Detection 188: position=(202,279), size=(80x35), confidence=0.326\n",
      "  Detection 189: position=(108,279), size=(80x35), confidence=0.325\n",
      "  Detection 190: position=(384,169), size=(80x35), confidence=0.325\n",
      "  Detection 191: position=(552,171), size=(80x35), confidence=0.325\n",
      "  Detection 192: position=(634,170), size=(80x35), confidence=0.325\n",
      "  Detection 193: position=(321,303), size=(80x35), confidence=0.324\n",
      "  Detection 194: position=(553,507), size=(80x35), confidence=0.323\n",
      "  Detection 195: position=(76,398), size=(168x73), confidence=0.323\n",
      "  Detection 196: position=(60,501), size=(80x35), confidence=0.323\n",
      "  Detection 197: position=(181,201), size=(80x35), confidence=0.323\n",
      "  Detection 198: position=(598,196), size=(168x73), confidence=0.322\n",
      "  Detection 199: position=(346,508), size=(80x35), confidence=0.322\n",
      "  Detection 200: position=(614,238), size=(80x35), confidence=0.321\n",
      "  Detection 201: position=(174,333), size=(492x215), confidence=0.321\n",
      "  Detection 202: position=(95,201), size=(80x35), confidence=0.320\n",
      "  Detection 203: position=(173,204), size=(138x60), confidence=0.319\n",
      "  Detection 204: position=(27,55), size=(168x73), confidence=0.319\n",
      "  Detection 205: position=(260,198), size=(80x35), confidence=0.318\n",
      "  Detection 206: position=(429,508), size=(80x35), confidence=0.318\n",
      "  Detection 207: position=(640,304), size=(80x35), confidence=0.317\n",
      "  Detection 208: position=(452,238), size=(80x35), confidence=0.317\n",
      "  Detection 209: position=(439,170), size=(80x35), confidence=0.317\n",
      "  Detection 210: position=(135,201), size=(80x35), confidence=0.316\n",
      "  Detection 211: position=(312,512), size=(197x86), confidence=0.316\n",
      "  Detection 212: position=(68,302), size=(80x35), confidence=0.315\n",
      "  Detection 213: position=(714,200), size=(80x35), confidence=0.314\n",
      "  Detection 214: position=(389,510), size=(80x35), confidence=0.313\n",
      "  Detection 215: position=(532,185), size=(80x35), confidence=0.313\n",
      "  Detection 216: position=(550,199), size=(80x35), confidence=0.312\n",
      "  Detection 217: position=(312,195), size=(168x73), confidence=0.312\n",
      "  Detection 218: position=(33,186), size=(80x35), confidence=0.311\n",
      "  Detection 219: position=(291,121), size=(168x73), confidence=0.309\n",
      "  Detection 220: position=(330,299), size=(197x86), confidence=0.308\n",
      "  Detection 221: position=(1,298), size=(197x86), confidence=0.306\n",
      "  Detection 222: position=(149,302), size=(80x35), confidence=0.306\n",
      "  Detection 223: position=(667,37), size=(80x35), confidence=0.305\n",
      "  Detection 224: position=(89,204), size=(138x60), confidence=0.304\n",
      "  Detection 225: position=(650,66), size=(109x47), confidence=0.302\n",
      "  Detection 226: position=(477,304), size=(80x35), confidence=0.301\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ASSIGNMENT 2: Multiple Detections on coca_multi.png\n",
    "# Using template matching with the same template from Item 1 (like 01.Templates.ipynb Cell 6)\n",
    "# =============================================================================\n",
    "\n",
    "img_rgb, img_gray, img_bgr = load_image('images/coca_multi.png')\n",
    "detections = detect_logo(img_bgr, img_gray, variants, single_detection=False)\n",
    "\n",
    "print(f\"Detected {len(detections)} logo instances in coca_multi.png\")\n",
    "for i, det in enumerate(detections):\n",
    "    x, y, w, h, score = det\n",
    "    print(f\"  Detection {i+1}: position=({x},{y}), size=({w}x{h}), confidence={score:.3f}\")\n",
    "\n",
    "plot_single_result(img_rgb, detections, \"ASSIGNMENT 2: coca_multi.png (Template Matching)\", 'results/Figure_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf815d5",
   "metadata": {},
   "source": [
    "3. Generalize the algorithm from item 2 for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1b10e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCA-COLA-LOGO.jpg: 1 single detection, 72 multiple detections\n",
      "coca_logo_1.png: 1 single detection, 7 multiple detections\n",
      "coca_logo_2.png: 1 single detection, 9 multiple detections\n",
      "coca_multi.png: 1 single detection, 226 multiple detections\n",
      "coca_retro_1.png: 1 single detection, 130 multiple detections\n",
      "coca_retro_2.png: 1 single detection, 55 multiple detections\n",
      "logo_1.png: 1 single detection, 66 multiple detections\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ASSIGNMENT 3: Generalized Algorithm for All Images\n",
    "# Generalizes the template matching algorithm from Item 2 for all images\n",
    "# =============================================================================\n",
    "\n",
    "results_single = []\n",
    "results_multi = []\n",
    "\n",
    "for img_name in image_files:\n",
    "    img_rgb, img_gray, img_bgr = load_image(os.path.join(IMAGES_DIR, img_name))\n",
    "    \n",
    "    # Single detection: template matching with strict threshold\n",
    "    bbox, method, score = detect_logo(img_bgr, img_gray, variants, single_detection=True)\n",
    "    det_single = [(bbox[0], bbox[1], bbox[2], bbox[3], score)] if bbox else []\n",
    "    results_single.append((img_rgb, det_single, img_name))\n",
    "    \n",
    "    # Multiple detection: template matching with more permissive threshold (Item 2 algorithm)\n",
    "    det_multi = detect_logo(img_bgr, img_gray, variants, single_detection=False)\n",
    "    results_multi.append((img_rgb, det_multi, img_name))\n",
    "    \n",
    "    print(f\"{img_name}: {len(det_single)} single detection, {len(det_multi)} multiple detections\")\n",
    "\n",
    "plot_results_grid(results_single, \"ASSIGNMENT 3: Single Detection (Template Matching)\", 'results/Figure_3a_single.png')\n",
    "plot_results_grid(results_multi, \"ASSIGNMENT 3: Multiple Detection (Template Matching)\", 'results/Figure_3b_multi.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
